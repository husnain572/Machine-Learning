{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Scaling vs Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Both are use in ML in pre-processing\n",
    "2. Both can be use in data analytics\n",
    "3. Both are inter changable\n",
    "## **Scaling**:\n",
    "- The primary of scaling is to change the range os the data without altering the shape of its distribution. This  is important for the algorithms that are sensitive to the scale of the data, such as support vector machines(SVMs) or k-nearest neigbhbors(K-NN).\n",
    "## Methods: \n",
    "- Common methods of scaling include Min-Max scaling and z-score standardization(also known as standard scaling). Min-Max scaling transforms the data to fit within the specific range (typically 0 to 1), while Z-score standardization rescales the data to have a mean of 0 and standard deviation of 1.\n",
    "## Use Case:\n",
    "- Scaling is used when you need to compare data that is measured  on different scale, like comparing income to the numberof years of education.\n",
    "\n",
    "## **Normalizaton**:\n",
    "- Normalization adjusts the scale of the data but also  changes the shapre of its distribution. It is used to transform features to be on a similar scale. This includes transforming skewed data to approximate a normal distribution.\n",
    "## Method:\n",
    "- Common methods of normalization include log transformation, square root transformation, and Box-Cox transformation. These techniques can be used to reduce the skewnnessof the data.\n",
    "## Use Case:\n",
    "- Normalization is typically used when the dataset has skewed distributions and the algorithms used assumed normality of feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
